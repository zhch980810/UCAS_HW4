{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "decent-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split, DataLoader,Dataset\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "import gensim\n",
    "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence,pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "mechanical-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model=gensim.models.keyedvectors.load_word2vec_format('data/Comment/wiki_word2vec_50.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "finnish-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wiki和train没有包含关系\n",
    "#制作新数据集的词表\n",
    "words=[]\n",
    "with open('data/Comment/train.txt','r',encoding='UTF-8') as f:\n",
    "    lines=f.readlines()\n",
    "    for line in lines:\n",
    "        line_words = re.split(r'[\\s]', line)[1:-1]\n",
    "        for w in line_words:\n",
    "                words.append(w)\n",
    "\n",
    "words=sorted(set(words))#words是新的词表 ,需要word2ix,ix2word,和ix2vector\n",
    "word2ix={w:i+1 for i,w in enumerate(words)}\n",
    "ix2word={i+1:w for i ,w in enumerate(words)}\n",
    "word2ix['UNK']=0\n",
    "ix2word[0]='UNK'\n",
    "#UNK的含义是test和valid还没见过的集合中可能存在的未知数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "subjective-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "#先做一个基础数据集，它每个数据的长短不同，是那句话的长度，不能直接进dataloader\n",
    "class MyComment2(Dataset):\n",
    "    def __init__(self,path,word2ix):\n",
    "        self.word2ix=word2ix\n",
    "        self.labels=[]\n",
    "        self.comments=[]\n",
    "        with open(path,'r',encoding='UTF-8') as f:\n",
    "            lines=f.readlines()\n",
    "        for line in lines:\n",
    "            line=re.split(r'[\\s]',line)\n",
    "            self.labels.append(int(line[0]))\n",
    "            comment=line[1:-1]\n",
    "            comment_dig=[]\n",
    "            for i in comment:\n",
    "                try:\n",
    "                    comment_dig.append(word2ix[i])\n",
    "                except KeyError:\n",
    "                    comment_dig.append(0)\n",
    "            self.comments.append(torch.tensor(comment_dig,dtype=torch.int64))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self,num):\n",
    "        return self.comments[num],self.labels[num]\n",
    "    \n",
    "#精加工，返回的是补齐的tensor格式，comment和lengths进lstm\n",
    "def mycollate_fn(batch):\n",
    "    batch.sort(key=lambda x:len(x[0]),reverse=True)\n",
    "    lengths=[len(i[0]) for i in batch]\n",
    "    labels=torch.tensor([i[1] for i in batch],dtype=torch.long)\n",
    "    comment=[i[0] for i in batch]\n",
    "    comment=pad_sequence(comment,batch_first=True,padding_value=0)\n",
    "    return comment,lengths,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fourth-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#制作dataloader\n",
    "train_ds=MyComment2(path='data/Comment/train.txt',word2ix=word2ix)\n",
    "train_loader=DataLoader(train_ds,40,shuffle=True,collate_fn=mycollate_fn)\n",
    "#train_loader是经过embed的向量，并且不打算修改embed参数，直接训练一个lstm\n",
    "\n",
    "val_ds=MyComment2(path='data/Comment/validation.txt',word2ix=word2ix)\n",
    "val_loader=DataLoader(val_ds,40,shuffle=True,collate_fn=mycollate_fn)\n",
    "test_ds=MyComment2(path='data/Comment/test.txt',word2ix=word2ix)\n",
    "test_loader=DataLoader(test_ds,40,shuffle=True,collate_fn=mycollate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "understanding-plant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53338\n"
     ]
    }
   ],
   "source": [
    "print(len(word2ix))\n",
    "#ix-->WORD-->word2vec_model.key_to_index[...]-->model.getvector(.)\n",
    "#没有的词初始化为0向量\n",
    "my_weight=[]\n",
    "for i in range(len(word2ix)):\n",
    "    word=ix2word[i]\n",
    "    try:\n",
    "        index=word2vec_model.key_to_index[word]\n",
    "        my_weight.append(torch.tensor(word2vec_model.get_vector(index)))\n",
    "    except KeyError:\n",
    "        my_weight.append(torch.zeros(50))\n",
    "my_weight=torch.stack(my_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "consecutive-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "#可选是否要把长度信息提供给lstm\n",
    "#可选使用hidden做分类还是output做分类\n",
    "#可选是否对初始的矩阵进行调整\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class FeelingCatcher(nn.Module):\n",
    "            \n",
    "    def __init__(self,hid_dim,n_layers, renew ,length_change, who):\n",
    "        super(FeelingCatcher,self).__init__()\n",
    "        \n",
    "        if who!='hidden'and who!='output' and who!='output_last':\n",
    "            print('who是指用谁进行判断，请在hidden，output，output_last中做选择')\n",
    "            return -1\n",
    "        \n",
    "        self.embedding=nn.Embedding.from_pretrained(my_weight)\n",
    "        self.embedding.requires_grad_=renew  #决定是否学习新的词向量\n",
    "        \n",
    "        self.lstm=nn.LSTM(50,hid_dim,n_layers,batch_first=True,dropout=0.2)\n",
    "        #lstm——hidden——【batchsize，hid_dim,n_layers】\n",
    "        #lstm----outputs----[batchsize,seq_len,hid_dim]\n",
    "        \n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "        self.fc1=nn.Linear(hid_dim,256)\n",
    "        self.fc2=nn.Linear(256,64)\n",
    "        self.fc3=nn.Linear(64,2)\n",
    "        \n",
    "        self.length_change=length_change# 是否接受不定长输入\n",
    "        self.who=who #选谁判断\n",
    "\n",
    "        \n",
    "    def forward(self,comments,lengths):\n",
    "        out=self.embedding(comments)\n",
    "        \n",
    "        if self.length_change:\n",
    "            out=pack_padded_sequence(out,lengths,batch_first=True)\n",
    "            out,hidden=self.lstm(out)  #hidden[n_layers,batchsize,,hidden]\n",
    "            out=pad_packed_sequence(out,batch_first=True)[0] #[batch,seq_len,hidden_dim]\n",
    "        else:\n",
    "            out,hidden=self.lstm(out)\n",
    "            \n",
    "        if self.who=='output_last':\n",
    "            out=out[:,-1,:]   #[batch,hidden_dim]\n",
    "\n",
    "        if self.who=='output':\n",
    "            out=torch.sum(out,dim=1)/out.shape[1]\n",
    "\n",
    "        if self.who=='hidden':\n",
    "            out=hidden[1][-1,:,:]\n",
    "            \n",
    "            \n",
    "        out=self.dropout(torch.tanh(self.fc1(out)))\n",
    "        out=self.dropout(torch.tanh(self.fc2(out)))\n",
    "        out=self.fc3(out)\n",
    "        \n",
    "        return out   #[batch,2]\n",
    "    \n",
    "    \n",
    "    def step(self, batch, device):  #返回一个batch的loss和acc\n",
    "        comments, lengths, labels = batch\n",
    "        comments=comments.to(device)\n",
    "        labels=labels.to(device)\n",
    "        \n",
    "        out = self(comments,lengths)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        return {'loss': loss, 'acc': acc}\n",
    "    \n",
    "        \n",
    "    def evaluate(self, loader,device):     #打包评价这轮valloader中loss和acc的平均值\n",
    "        outputs = [self.step(batch,device) for batch in loader]\n",
    "        losses = [x['loss'] for x in outputs]\n",
    "        losses = torch.stack(losses).mean()\n",
    "        accs = [x['acc'] for x in outputs]\n",
    "        accs = torch.stack(accs).mean()\n",
    "        return {'loss': losses.item(), 'acc': accs.item()}\n",
    "\n",
    "    \n",
    "    def epoch_end(self, epoch, train_loss,result):  #打印训练，测试上的loss，acc\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, train_loss, result['loss'], result['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "magnetic-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, max_lr, model, train_loader, val_loader, grad_clip=None, opt_func=torch.optim.Adam,device='cpu'):\n",
    "    \n",
    "    history_train = []\n",
    "    history_val=[]\n",
    "    #history_lr=[]\n",
    "    optimizer = opt_func(model.parameters(), max_lr,weight_decay = 1e-4)\n",
    "    #sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            loss = model.step(batch,device)['loss']\n",
    "            loss.backward()\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            #history_lr.append(get_lr(optimizer))\n",
    "            #sched.step()\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            result = model.evaluate( val_loader,device)\n",
    "            model.epoch_end(epoch,loss.item(),result)\n",
    "            \n",
    "            history_train.append(loss.item())\n",
    "            history_val.append(result['loss'])\n",
    "            \n",
    "    return history_train,history_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "blind-daniel",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-70a74a207d47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFeelingCatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrenew\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlength_change\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwho\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'output_last'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhistory_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhistory_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-67-5674fb29707c>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(epochs, max_lr, model, train_loader, val_loader, grad_clip, opt_func, device)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_value_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python385\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python385\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model=FeelingCatcher(128,3,renew=True,length_change=True,who='output_last').to(device)\n",
    "\n",
    "history_train,history_val=fit(1,0.01,model,train_loader,val_loader,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "alien-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def predict(x,path='comment.pth'):\n",
    "    comment=x.split('，')\n",
    "    seg=[' '.join(jieba.cut(i,cut_all=False)) for i in comment]\n",
    "    num_seg=[]\n",
    "    for i in seg:\n",
    "        j=i.split(' ')\n",
    "        for l in j:\n",
    "            try:\n",
    "                num_seg.append(word2ix[l])\n",
    "            except KeyError:\n",
    "                num_seg.append(0)\n",
    "    num_seg=torch.tensor(num_seg)\n",
    "    num_seg=num_seg.unsqueeze(0)\n",
    "    seg_len=[num_seg.size()[1]]\n",
    "\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model=FeelingCatcher(128,3,renew=True,length_change=True,who='output_last').to(device)\n",
    "    model.load_state_dict(torch.load(path,map_location='cpu'))\n",
    "\n",
    "    pred=torch.argmax(model(num_seg,seg_len))\n",
    "    if pred:\n",
    "        print(x,'\\n','是差评55555')\n",
    "    else:\n",
    "        print(x,'\\n','是好评！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ranking-recruitment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整部片子全是变态。一个不良少女，一个弱智杀手，一个丧心病狂的警察。我怎么会看这么毁三观的片子。 \n",
      " 是差评55555\n",
      "------------------------------------------------------------------------\n",
      "里昂只有一颗盆栽，不善言辞，爱喝牛奶。他不像，却真正是一个杀手。玛蒂达的到来，是包袱，也给里昂带来了生机。不过这种设定，注定是悲剧收场。里昂死后，玛蒂达将他盆栽的种子落地生根，里昂终于不再每日拿着手枪在椅子上不安地入睡，他落地了。娜塔莉波特曼太灵了，玛蒂达是如此特别。 \n",
      " 是好评！\n"
     ]
    }
   ],
   "source": [
    "predict('整部片子全是变态。一个不良少女，一个弱智杀手，一个丧心病狂的警察。我怎么会看这么毁三观的片子。')\n",
    "print('------------------------------------------------------------------------')\n",
    "predict('里昂只有一颗盆栽，不善言辞，爱喝牛奶。他不像，却真正是一个杀手。玛蒂达的到来，是包袱，也给里昂带来了生机。不过这种设定，注定是悲剧收场。里昂死后，玛蒂达将他盆栽的种子落地生根，里昂终于不再每日拿着手枪在椅子上不安地入睡，他落地了。娜塔莉波特曼太灵了，玛蒂达是如此特别。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "familiar-intranet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['评分超高的影片', '看完毫无感觉', '明明很一般啊', '嗯', '一定是我不懂得欣赏'] \n",
      " 是好评！\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python385",
   "language": "python",
   "name": "python385"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
